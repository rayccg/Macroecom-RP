---
title: "Effects of Fiscal Policy on Unemployment and Output in Australia: a Bayesian SVAR Approach"
author: "Ray Gomez"

execute:
  echo: false
  
bibliography: references.bib
---

> **Abstract.** This proposed study attempts to estimate the effects of fiscal policy instruments on unemployment and output in Australia, using Bayesian Structural Vector Autoregression (BSVAR) models that account for conditional heteroskedasticity. Impulse response functions are used for measuring these effects.
>
> **Keywords.** Fiscal policy, unemployment, GDP, Australia impulse response, COVID-19, R

## Introduction

In many countries, fiscal policy is viewed as a direct means of achieving inclusive development objectives, frequently articulated as twin goals of sustained growth and low poverty (often achieved through low unemployment). However, fiscal policy shocks often occur in multifaceted ways, with simultaneous changes in both revenue and expenditure-side elements of the fiscal balance. This makes it difficult to disentangle the effects of any one particular policy shock.

This study attempts to identify the effect of the following fiscal policy instrument: tax policy, public investment expenditures, and social transfers, on both unemployment and output in the Australian context, using a Bayesian Structural Vector Autoregression (BSVAR) approach.

The analysis is inspired by the work of @abubakar2016dynamic, which used an SVAR approach to estimate the effect of public expenditures and public revenues on Nigerian output and unemployment, @lenza2022estimate which proposes an estimation procedure for VARs after the COVID-19 pandemic, and the common stochastic volatility model framework of @jacquier1994bayesian. This study extends their work by using a more extensive data set, containing disaggregated revenue and expenditure variables, the inclusion of monetary policy control variables, and by applying the respective volatility models into a structural analysis.

The author acknowledges and deeply thanks Dr. Tomasz Wo≈∫niak for generously providing tireless support, guidance, and baseline `R` codes on which the full script of the analysis was built upon.

## Data sources

Quarterly data from Q1 1990 to Q4 2022 was sourced from the Australian Bureau of Statistics (ABS) and the Reserve Bank of Australia (RBA). These were extracted using the `readabs` and `readrba` packages in `R`.

Unemployment rate and nominal GDP data are viewed as the outcome variables while tax revenue, public gross fixed capital formation (i.e., public investment) and government social assistance payments (i.e., social subsidies) are the explanatory variables of interest. Government final consumption is included to control for the effects of less productive, more routine government spending. Lastly, non-tax revenue (which, arguably, have a less distortionary effect on markets compared to taxes) and monetary policy reflected through the cash rate target and M3 money supply are also controlled for to mitigate omitted variable issues.

Outcome variables:

-   Unemployment rate and nominal GDP

Explanatory variables of interest

-   Revenue: Tax revenue

-   Spending: Public gross fixed capital formation and social assistance payments

Control variables:

-   Other fiscal policies: non-tax (gross income less tax) revenue, government final consumption
-   Monetary policy: cash rate target and M3 money supply
-   External sector: real exchange rate

```{r libraries and globals, include = F}
rm(list=ls())
library(readrba)
library(readabs)
library(xts)
library(tseries)
library(urca)
library(FinTS)
library(rmarkdown)
library(corrplot)
library(parallel)
library(MASS)
library(coda)
library(sads)
library(truncnorm)
library(mvtnorm)
library(plot3D)
library(HDInterval)
library(RColorBrewer)
library(mgcv)

S.global = 5000
```

```{r data download, include = F}
#| message: false
#rba <- browse_rba_series(search_string = "")
#abs <- as.matrix(show_available_catalogues())
#remotes::install_github("mattcowgill/readrba")

## Monetary policy variables
# Cash interest rate
cash_rate.dl   <- read_rba(series_id = "FIRMMCRTD")
cashrate <- to.quarterly(xts(cash_rate.dl$value, cash_rate.dl$date), 
                         OHLC = FALSE)
# M3 Money supply
M3.dl   <- read_rba(series_id = "DMAM3S")
M3 <- to.quarterly(xts(M3.dl$value, M3.dl$date), 
                         OHLC = FALSE)

# real exchange rate (TWI)
# realTWI.dl   <- read_rba(series_id = "FRERTWI")
# realTWI <- to.quarterly(xts(realTWI.dl$value, realTWI.dl$date), 
                         # OHLC = FALSE)

# Unemployment rate: A84423092X 
# sadj A84423050A
unemp_rate.dl <- read_abs(series_id = "A84423050A")
unemp <- to.quarterly(xts(unemp_rate.dl$value, unemp_rate.dl$date), 
                           OHLC = FALSE)

# real GDP: A2302459A
# sadj A2304402X
realGDP.dl   <- read_abs(series_id = "A2304402X")
realGDP <- to.quarterly(xts(realGDP.dl$value, realGDP.dl$date), 
                         OHLC = FALSE)

# Nominal GDP: A2302467A
# nomGDP.dl <- read_abs(series_id = "A2302467A")
# nomGDP <- to.quarterly(xts(nomGDP.dl$value, nomGDP.dl$date), 
#                          OHLC = FALSE)

#summary(unemp_rate)
## Fiscal variables
# Total tax: A2301963V 
totaltax.dl <- read_abs(series_id = "A2302794K")
totaltax <- to.quarterly(xts(totaltax.dl$value, totaltax.dl$date), 
                         OHLC = FALSE)

# Non-tax revenue: Gross income (A2302106V) - total tax 
govgrossinc.dl <- read_abs(series_id = "A2302742J")
govgrossinc <- to.quarterly(xts(govgrossinc.dl$value, govgrossinc.dl$date), 
                         OHLC = FALSE)

nontax <- govgrossinc - totaltax

# Public gross fixed capital formation: A2302555A
pubinv.dl <- read_abs(series_id = "A2304065W")
pubinv <- to.quarterly(xts(pubinv.dl$value, pubinv.dl$date), 
                         OHLC = FALSE)

# Social assistance benefits payments: A2301976F
pubtrans.dl <- read_abs(series_id = "A2301976F")
pubtrans <- to.quarterly(xts(pubtrans.dl$value, pubtrans.dl$date), 
                         OHLC = FALSE)

# Government final consumption: A2302527T
pubcons.dl <- read_abs(series_id = "A2304036K")
pubcons <- to.quarterly(xts(pubcons.dl$value, pubcons.dl$date),
                         OHLC = FALSE)

```

##### Table 1. Data from ABS and RBA {style="text-align: center;"}

```{r data merge and log transform, echo = F}
# Merge data into one matrix
Y.df  <- na.omit(merge(unemp, realGDP , totaltax, nontax, pubinv, 
                       pubtrans, pubcons, cashrate, M3))

varname_vec <- c("Unemployment rate", "Real GDP", "Tax revenue", "Non-tax revenue", "Gov't GFCF",
                    "Social benefits payments", "Gov't consumption", "Cash rate target", "M3 supply")
colnames(Y.df) <- varname_vec

Y.df <- Y.df[1:132,]
# Transform into natural logs
lnY.df <- log(Y.df)

date <- as.vector(index(cashrate))[1:132]
T <- length(date)

paged_table(as.data.frame(round(head(lnY.df,4),2)))
paged_table(as.data.frame(round(tail(lnY.df,4),2)))

# data for input to estimation function 
lnY.df_num <- c()

for (i in 1:ncol(lnY.df)){
  lnY.df_num <- cbind(lnY.df_num, as.numeric(lnY.df[,i]))
}

# Return unemployment rate and cash rate target to non-log terms
lnY.df_num[,1] <- Y.df[,1] 
lnY.df_num[,8] <- Y.df[,8] 

```

# 

As seen in the figures below, Australian GDP output has been on a relatively steady, upward trend since 1990, along with fiscal revenue and spending. These have, by and large, been accommodated by declining interest rates and an expanding money supply. Overall, these have been associated with a downward, albeit volatile trajectory in the unemployment rate. Seasonally-adjusted versions of all variables are used in the analysis, except for social benefit payments which only has an unadjusted version in the ABS website. The data, except for unemployment rate and cash rate target, are transformed into natural log terms during the analysis as to interpret the coefficients in the structural matrix as elasticities.

##### Figure 1. Time series plots: 1990 Q1 to 2022 Q4, original values {style="text-align: center;"}

```{r, echo = F}
colvec <- c("turquoise4","turquoise4", "plum4", "plum4", "plum4", "plum4", "plum4","firebrick3","firebrick3")
par(mfrow=c(3,3), mar=c(2,2,2,2))
for (i in 1:9){
  plot(date, y = Y.df[,i], type = "l", 
       main = paste(varname_vec[i]), ylab = "", xlab = "",
       col = colvec[i], lwd = 1.5,
       ylim = c(min(Y.df[,i]),max(Y.df[,i])))
}

```

## Preliminary data analysis

### ACF and PACF analysis

Across variables (which have been transformed into natural log terms), ACF plots show a strong, positive, and gradually decaying autocorrelation structure. Moreover the PACF plots show a near-one value at the first lag and statistically zero values at higher lag orders, indicating that these macroeconomic variables follow a random walk with drift process and are thereby unit-root non-stationary.

###### Figure 2.1. Autocorrelation function (ACF) plots of variables in log terms {style="text-align: center;"}

```{r ACF plots, echo = FALSE}
par(mfrow=c(3,3), mar=c(2,2,2,2))
for (i in 1:9){
  Acf(lnY.df[,i], main = "", ylim = c(-0.2,1))
  title(main = paste(varname_vec[i]), line = 0.5)
} 
```

###### Figure 2.2. Partial autocorrelation function (PACF) plots of variables in log terms {style="text-align: center;"}

```{r PACF plotsPACF analysis, echo = FALSE}
par(mfrow=c(3,3), mar=c(2,2,2,2))
for (i in 1:9){
  pacf(lnY.df[,i], main = "", ylim = c(-0.2,1))
  title(main = paste(varname_vec[i]), line = 0.5)
} 

```

### Augmented Dickey-Fuller test for unit roots

```{r, warning = F, echo = T}
#| code-fold: true
#| code-summary: "Show code"
# Thank you Zhang, Hanwen (2023) for providing the cleaner version of the ADF test code
df <- lnY.df_num

# ADF test at levels
adf <- as.data.frame(matrix(nrow=ncol(df),ncol=3,NA))
rownames(adf) <- varname_vec
colnames(adf) <- c("Dickey-Fuller","Lag order", "p-value")

for (i in 1: ncol(df)){
  adf_tmp                 <- adf.test(df[,i])
  adf[i,"Dickey-Fuller"]  <-  round(as.numeric(adf_tmp[1]),3)
  adf[i,"Lag order"]      <-  as.numeric(adf_tmp[2])
  adf[i,"p-value"]        <-  round(as.numeric(adf_tmp[4]),3)
}

# ADF test at first difference
adf.diff <- as.data.frame(matrix(nrow=ncol(df),ncol=3,NA))
rownames(adf.diff) <- varname_vec
colnames(adf.diff) <- c("Dickey-Fuller","Lag order", "p-value")

for (i in 1: ncol(df)){
  tmp <- adf.test(diff(df[,i]))
  adf.diff[i,"Dickey-Fuller"] <-  round(as.numeric(tmp[1]),3)
  adf.diff[i,"Lag order"]<-  as.numeric(tmp[2])
  adf.diff[i,"p-value"] <-  round(as.numeric(tmp[4]),3)
}

# ADF test at second difference
adf.diff2 <- as.data.frame(matrix(nrow=ncol(df),ncol=3,NA))
rownames(adf.diff2) <- varname_vec
colnames(adf.diff2) <- c("Dickey-Fuller","Lag order", "p-value")

for (i in 1: ncol(df)){
  tmp <- adf.test(diff(diff(df[,i])))
  adf.diff2[i,"Dickey-Fuller"] <-  round(as.numeric(tmp[1]),3)
  adf.diff2[i,"Lag order"]<-  as.numeric(tmp[2])
  adf.diff2[i,"p-value"] <-  round(as.numeric(tmp[4]),3)
}

```

##### Table 2.1. ADF test results: levels {style="text-align: center;"}

```{r, warning = F, echo = F}
paged_table(as.data.frame(adf))
```

##### Table 2.2. ADF test results: first difference {style="text-align: center;"}

```{r, warning = F, echo = F}
paged_table(as.data.frame(adf.diff))
```

##### Table 2.3. ADF test results: second difference {style="text-align: center;"}

```{r, warning = F, echo = F}
paged_table(as.data.frame(adf.diff2))
```

# 

Augmented Dickey-Fuller tests were performed on all variables to determine stationarity and orders of integration, using the `adf.test` function in `R`. As seen in the results in Tables 2.1 to 2.3, the null hypothesis that a unit roots exists was rejected for ADF tests at the first difference for all variables except M3 supply, indicating that these variables have one unit-root. It took another differencing for M3 to yield a significant result, indicating an order of integration of 2.

## Model specification

The use of fiscal policy to stimulate or regulate the economy follows the Keynesian approach generally practiced around the world (but especially in developing countries) to navigate economies through downturns or potential episodes of overheating, particularly in the short-run. The proposed model aims to investigate the effectiveness of these fiscal measures in managing growth and unemployment.

Estimation proceeds through the following reduced form model:

$$
\begin{align}
Y &= XA + U\\
U| X &\sim _{iid} MN_{T\times N}( 0, \Sigma, \Omega)\\
\Sigma &= B_0^{-1}B_0^{-1'}
\end{align}
$$ where matrix $Y$ contains vectors of endogenous variables $y_t$:

$$y_t=\begin{pmatrix} unemp_t &= \text{unemployment rate}
\\ realgdp_t &= \text{real GDP}
\\ totaltax_t  &= \text{tax revenue}
\\ nontax_t  &= \text{non-tax revenue}
\\ pubinv_t  &= \text{government gross fixed capital formation}
\\ pubtrans_t  &= \text{social assitance and benefits payments}
\\ pubcons_t  &= \text{government final consumption}
\\ cashrate_t  &= \text{cash rate target}
\\ M3_t  &= \text{M3 money supply}
\end{pmatrix}$$

and $B_0^{-1}=B$ is the matrix of contemporaneous effects between variables.

## Estimation and identification

### Basic model

For the basic model, the column-wise covariance matrix is set as $\Omega=I_N$. The likelihood function is expressed as:

$$P(Y,X|A,\Sigma) \propto det(\Sigma)^{-\frac{T}{2}} exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(Y-XA)'(Y-XA) \right] \right\}\\$$ $$=det(\Sigma)^{-\frac{T}{2}} exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\hat{A})'X'X(A-\hat{A}) \right] \right\} exp \left\{-\frac{1}{2} tr \left[\Sigma^{-1}(Y-X \hat{A})'(Y-X \hat{A}) \right] \right\}$$

where the maximum likelihood estimators are expressed as:

$$\hat{A} = (X'X)^{-1}X'Y $$ $$\hat{\Sigma} = \frac{1}{T} (Y-X \hat{A})'(Y-X \hat{A})$$

The prior for $A$ and $\Sigma$ is assumed to be a matrix-normal inverse Wishart distribution that follows the Minnesota specification of Doan, Litterman & Sims (1984):

$$p(A,\Sigma) =MNIW(\underline{A},\underline{S},\underline{V},\underline{\nu})$$

$$A|\Sigma\sim MN_{K\times N}(\underline{A},\Sigma,\underline{V})$$ $$\Sigma\sim IW_N(\underline{S},\underline{\nu})$$

where:

$$\underline{A} = [0_{N \times 1} \quad I_N \quad 0_{N \times (p-1)N}]'$$ $$\underline{V} = diag([\kappa_2 \quad \kappa_1 (p^{-2} \otimes I_N)])$$

This leads to the following posterior distribution, from which the basic reduced form model will be estimated:

$$P(A|Y,X,\Sigma)=MN_{K\times N}(\bar{A},\Sigma,\bar{V})$$ $$P(\Sigma|Y,X)=IW_N(\bar{S},\bar{\nu})\\$$

$$\bar{V}=(X'X+\underline{V}^{-1})^{-1}$$ $$\bar{A}=\bar{V}(X'Y+\underline{V}^{-1}\underline{A})$$ $$\bar{\nu}=T+\underline{\nu}$$ $$\bar{S}=\underline{S}+Y'Y+\underline{A}'\underline{V}^{-1}\underline{A}-
\bar{A}'\bar{V}^{-1}\bar{A}$$

### COVID volatility model

For the COVID volatility model, which explicitly attempts to model conditional heteroskedasticity beginning from the onset of the pandemic, the column-wise covariance matrix is set to $\Omega=\text{diag}(H^2)$, where $H$ is a vector of COVID volatility variables:

$$H=[1\quad...\quad1 \quad h_0\quad h_1\quad h_2\quad 1+(h_2-1)\rho\quad 1+(h_2-1)\rho^2\quad...]'$$

Intuitively, volatility variables for periods before COVID are set to unity. Heightened volatility during the first three quarters of COVID are parameterized, before they are assumed to decay gradually beginning at the fourth quarter since the pandemic's onset.

These COVID volatility parameters $\theta=(h_0\quad h_1\quad h_2\quad\rho)$ are estimated from their own marginal posterior as proposed by Lenza & Primiceri (2020):

$$P(\theta|Y,X,\underline{\gamma})\propto P(Y,X|\theta,\underline{\gamma})P(\theta|\underline{\gamma})$$

where the likelihood function is given as:

$$P(Y,X|\theta,\underline{\gamma})\propto \Bigg(\prod^T_{t=1}h_t^{-N}\Bigg)||\underline{V}||^{\frac{N}{2}}||\underline{S}||^{\frac{\underline{\nu}}{2}}||\tilde{X}'\tilde{X}+\underline{V}^{-1}||^{\frac{N}{2}}\\$$

$$||\underline{S}+\hat{\tilde{E}}'\hat{\tilde{E}}+(\hat{\tilde{A}}-\tilde{Y}+\tilde{X}\underline{A})'\underline{V}^{-1}(\hat{\tilde{A}}-\tilde{Y}+\tilde{X}\underline{A})||^{-\frac{T-p+\underline{\nu}}{2}}$$

where $\tilde{X}_t=\frac{(1,Y_t,...,Y_{t-p})'}{h_t}$, $\tilde{\hat{A}}=(\tilde{X}'\tilde{X}-\underline{V}^{-1})^{-1}$, and $\underline{\gamma}=(\underline{A},\underline{S},\underline{V},\underline{\nu})$

and the priors are assumed to be: $h_0,h_1,h_2\sim Pareto(1,1)$ and $\rho\sim Beta(3,1.5)$

Sampling from the above posterior is conducted via the following Metropolis MCMC algorithm:

1.  Initialize $\theta$ at the posterior mode which was located via numerical optimization

2.  Draw candidate $\theta^{*}$ from $N_4(\theta^{(s-1)},cW)$, where $W$ is the inverse Hessian of the negative log posterior of $\theta$ at the mode, which is also calculated computationally, and $c$ is a scaling factor.

3.  Set:

$$\theta^{(s)}=  \begin{cases}
        \theta^* & \text{with pr.} \quad \alpha^{(s)} 
        \\
        \\
        \theta^{(s-1)} & \text{with pr.} \quad 1-\alpha^{(s)} 
  \end{cases}$$

$$\alpha^{(s)} =\text{min}\Big[1,\frac{P(\theta^*|Y,X,\underline{\gamma})}{P(\theta^{(s-1)}|Y,X,\underline{\gamma})}\Big]$$

4.  Define $H^{(s)}$ matrix using $\theta^{(s)}$:

$$H^{(s)}=[1\quad...\quad1 \quad h_0^{(s)}\quad h_1^{(s)}\quad h_2^{(s)}\quad 1+(h_2^{(s)}-1)\rho^{(s)}\quad 1+(h_2^{(s)}-1)\rho^{(s)2}\quad...]'$$

Steps 2 to 4 are repeated $S_1+S_2$ times and $S_2$ draws are used to draw $A$ and $\Sigma$ from the following posterior distribution:

$$P(A|Y,X,\Sigma,H)=MN_{K\times N}(\bar{A},\Sigma,\bar{V})$$ $$P(\Sigma|Y,X,H)=IW_N(\bar{S},\bar{\nu})\\$$

$$\bar{V}=(X'\text{diag}(H^2)^{-1}X+\underline{V}^{-1})^{-1}$$ $$\bar{A}=\bar{V}(X'\text{diag}(H^2)^{-1}Y+\underline{V}^{-1}\underline{A})$$ $$\bar{\nu}=T+\underline{\nu}$$ $$\bar{S}=\underline{S}+Y'\text{diag}(H^2)^{-1}Y+\underline{A}'\underline{V}^{-1}\underline{A}-
\bar{A}'\bar{V}^{-1}\bar{A}$$

### Common stochastic volatility model

Lastly, for the common volatility model the column-wise covariance matrix is set to $\Omega=\text{diag}(\sigma^2)$, where $\sigma^2$ is a vector of conditional heteroskedasticity variables:

$$\sigma^2=[\exp(h_0)\quad\exp(h_1)\quad... \quad \exp(h_T)]'$$ These are estimated through the simple stochastic volatility model of @jacquier1994bayesian, where across each variable $y_t$, the common volatility is estimated through:

$$\tilde{y}=h+\tilde{\varepsilon}$$ $$Hh=h_0e_{1.T}+\sigma_vv$$ $$\tilde{\varepsilon}\sim \log \chi^2_1$$ $$v\sim\mathcal{N}_T(0_T,I_T)$$ where:

$$\tilde{y}=[\log(y_1-\mu_1(\alpha))^2\quad... \quad \log(y_T-\mu_T(\alpha))^2]'$$ $$h=[h_1\quad... \quad h_T]'$$ $$\tilde{\varepsilon}=[\log\varepsilon_1^2\quad ...\quad  \log\varepsilon_T^2]'$$ $$v=[v_1\quad... \quad v_T]'$$ $$e_{1.T}=[1\quad 0\quad... \quad 0]'$$

Under this framework, the likelihood function for the reduced form model is given by:

$$\begin{gather}
L(A,\Sigma|Y,X,\sigma^2) \propto det(\text{diag}(\sigma^2))^{-\frac{N}{2}} det(\Sigma)^{-\frac{T}{2}} exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(Y-XA)'\text{diag}(\sigma^2)^{-1}(Y-XA) \right] \right\} \\
\end{gather} $$

and the posterior distribution of $A$ and $\Sigma$ is the following matrix-normal inverse Wishart distribution: $$\begin{gather}
P(A,\Sigma|X,Y,\sigma^2) \propto L(A,\Sigma|Y,X,\sigma^2) \times  p(A|\Sigma,\sigma^2) \times P(\Sigma) 
\end{gather}$$

$$P(A|Y,X,\Sigma,\sigma^2)=MN_{K\times N}(\bar{A},\Sigma,\bar{V})$$ $$P(\Sigma|Y,X,\sigma^2)=IW_N(\bar{S},\bar{\nu})\\$$ with posterior parameters:

$$\begin{gather}
\bar{V} = (X'\text{diag}(\sigma^2)^{-1}X+\underline{V}^{-1})^{-1} \\ 
\\ \bar{A} = \bar{V}(X'\text{diag}(\sigma^2)^{-1}Y+\underline{V}^{-1}\underline{A}) \\ 
\\ \bar{S} = \underline{S} + Y'\text{diag}(\sigma^2)^{-1}Y + \underline{A}^{'}\underline{V}^{-1}\underline{A} - \bar{A}^{'}\bar{V}^{-1}\bar{A} \\ 
\\ \bar{\nu}= T + \underline{\nu}
\end{gather}$$

Obtaining draws from this posterior involves a Gibbs sampler, which follows the following algorithm:

1.  Initialize $h^{(0)}$, $s^{(0)}$, and $\sigma^{2(0)}_v$

2.  Draw $h_0^{(s)}\sim\mathcal{N}(\bar{h}_0,\bar{\sigma}^2_v)$ where: $\bar{\sigma}^2_v=(\underline{\sigma}^{-2}_h+\sigma^{-2}_v)$ and $\bar{h}_0=\bar{\sigma}^2_v(e'Hh)$

3.  Draw $\sigma_v^{2(s)}\sim\mathcal{IG2}(\bar{s},\bar{\nu})$

4.  Draw $s_t^{(s)}\sim\mathcal{Multiomial}(\{m\}^{10}_{m=1},\{Pr[s_t=m|\tilde{y},h_t^{(s)}]\}^{10}_{m=1})$ for all $s_t$ in $s$. These are used for the auxiliary normal mixture approximation of the chi-squared-1 distribution.

5.  Draw $h^{(s)}\sim\mathcal{N}_T(\bar{h},\bar{V}_h)$ where $\bar{V}_h=[\text{diag}(\sigma^2_s)^{-1}+\sigma^{-2}_vH'H]^{-1}$ and $\bar{h}=\bar{V}_h[\text{diag}(\sigma^2_s)^{-1}(\tilde{y}-\mu_s)+\sigma^{-2}_vh_0e_{1.T}]$

6.  Compute vector $\sigma^2=[\exp(h_1^{(s)})\quad...\quad \exp(h_T^{(s)})]$

7.  Draw $(A,\Sigma)\sim\mathcal{MNIW}(\bar{A},\bar{V},\bar{S},\bar{\nu})$ and use these to compute residuals $\tilde{y}$

Steps 2 to 7 are repeated for $S=S_1+S_2$ draws, where $S_1$ draws are discarded as burn-in and the latter $S_2$ draws are kept as posterior draws.

### Identification

Sign restrictions are used as the identification strategy for all models.

Given the reduced form model:

$$
\begin{align}
Y &= XA + U\\
U| X &\sim _{iid} MN_{T\times N}( 0, \Sigma, \Omega)\\
\end{align}
$$ For each draw from the reduced form posteriors, the candidate contemporaneous effects matrix is derived as: $$\tilde{B}=\tilde{B}^{-1}_0=\text{chol}(\Sigma)$$

Contemporaneous effects matrix $B$ is identified by searching for an appropriate rotation matrix $Q$ such that prescribed sign restrictions hold:

$$B = Q\tilde{B}$$

such that:

$$R_nf(\tilde{B}_0,\tilde{B}_+)e_n\geq0$$ $$f(\tilde{B}_0,\tilde{B}_+)=\Theta_0=\tilde{B}^{-1}_0$$

### Estimation with artificial data

To check the validity of the algorithms, two independent bi-variate random walk with drift processes were generated to simulate unit-root non-stationary macroeconomic variables.

```{r RW sample}
set.seed(1)
y1 <- arima.sim(model= list(order = c(0, 1, 0)), n=999, mean=1,sd=1)
y2 <- arima.sim(model= list(order = c(0, 1, 0)), n=999, mean=1,sd=1)
y <- cbind(y1,y2)

par(mfrow=c(1,2))
plot(y=y1, x = 1:length(y1), type = "l", col = "turquoise4", 
     lwd = 1.5, ylab = "", xlab = "", main = "RW with drift 1")
plot(y=y2, x = 1:length(y1), type = "l", col = "plum4", 
     lwd = 1.5, ylab = "",xlab = "", main = "RW with drift 2")
```

The estimation procedure for the basic model is implemented in `R` by the following function, `sign.basic`:

```{r Basic model estimation function, echo = TRUE}
#| code-fold: true
#| code-summary: "Show code"

# data = input data should be quarterly
# p = lags
# S = number of posterior draws
# sign restritions = Nx1 diagonal of R matrix
# k1 = kappa1
# k2 = kappa2, higher value, less shrinkage, more weight on prior
# shockvar = variable being shocked
# start date = start date of Y matrix
sign.basic <- function(data, p, S,  sign.restrictions,
                           k1=0.04, k2=100, start_date = c(1991,1), shockvar=5){
  # Define Y and X matrices
  ############################################################
  # N = no. of variables
  N = ncol(data)
  # p = no. of lags
  K = 1 + p*N
  # forecast horizon
  # h       = 8
  
  Y       = ts(data[(p+1):nrow(data),], start=start_date, frequency=4)
  X       = matrix(1,nrow(Y),1)
  # nrow(X)
  for (i in 1:p){
    X     = cbind(X,data[(p+1):nrow(data)-i,])
  }
  
  # Calculate MLE
  ############################################################
  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  
  # Specify prior distribution
  ############################################################
  kappa.1     = k1
  kappa.2     = k2
  kappa.3     = 1
  A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
  A.prior[2:(N+1),] = kappa.3*diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1
  
  # Matrix normal-inverse Wishart posterior parameters
  ############################################################
  V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))
  V.bar       = solve(V.bar.inv)
  A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
  nu.bar      = nrow(Y) + nu.prior
  S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
  S.bar.inv   = solve(S.bar)
  
  # Posterior draws 
  ############################################################
  # Draws from RF posterior
  # Draw Sigma from inverse wishart
  Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
  Sigma.posterior   = apply(Sigma.posterior,3,solve)
  Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
  
  # Draw A from matrix-variate normal
  A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
  
  ## Draw from SF posterior
  B0.posterior       = array(NA,c(N,N,S))
  Bplus.posterior       = array(NA,c(N,K,S))
  L                 = t(chol(V.bar))
  for (s in 1:S){
    # Draw B0
    cholSigma.s     = chol(Sigma.posterior[,,s])
    B0.posterior[,,s]= solve(t(cholSigma.s))
    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s
    # Draw Bplus
    Bplus.posterior[,,s] = B0.posterior[,,s]%*%t(A.posterior[,,s])
  }

  # Identification via sign restrictions 
  ############################################################
  
  # Generate corresponding R matrix
  R1 = diag(sign.restrictions)
  
  # Storage matrices for Q identified estimates
  i.vec <- c()
  Q.iden   = array(NA,c(N,N,S))
  B0.iden = array(NA,c(N,N,S))
  B1.iden = array(NA,c(N,K,S))
  A.iden = array (NA,c(K,N,S))
  
  for (s in 1:S){
    
    # pick-up a B0 from S
    B0.tilde <- B0.posterior[,,s]
    IR.0.tilde    = solve(B0.tilde)
    B1.tilde      = Bplus.posterior[,,s]
    #IR.1.tilde    = solve(B0.tilde)%*%B1.tilde%*%solve(B0.tilde)

    # Search for appropriate Q 
    sign.restrictions.do.not.hold = TRUE
    i=1
    while (sign.restrictions.do.not.hold){
      X           = matrix(rnorm(N^2),N,N)
      QR          = qr(X, tol = 1e-10)
      Q           = qr.Q(QR,complete=TRUE)
      R           = qr.R(QR,complete=TRUE)
      Q           = t(Q %*% diag(sign(diag(R))))
      B0          = Q%*%B0.tilde
      B1          = Q%*%B1.tilde
      B0.inv      = solve(B0)
      check       = prod(R1 %*% B0.inv %*% diag(N)[,shockvar] >= 0)
      A           = t(solve(B0)%*%B1)

      if (check==1){sign.restrictions.do.not.hold=FALSE}
      i=i+1
    }
    i.vec <- c(i.vec, i)
    Q.iden[,,s] <- Q
    B0.iden[,,s] <- B0
    B0.mean <- apply(B0.iden,1:2,mean)
    B1.iden[,,s] <- B1
    B1.mean <- apply(B1.iden,1:2,mean)
    A.iden[,,s] <- A
    A.mean <- apply(A.iden,1:2,mean)
    

  }
  re <- list("i" = i.vec, "Q" = Q.iden, "B0"= B0.iden, "B0.mean" = B0.mean,
             "Bplus"= B1.iden, "Bplus.mean" = B1.mean, "A" = A.iden, "A.mean" = A,
             "A.posterior"=A.posterior, "Sigma.posterior"=Sigma.posterior)
  return(re)
}

```

Applying the `sign.basic` function on the artificial returns the following matrices as estimates for the posterior means of $A$ and $\Sigma$. These results are appropriate given the independent bivariate random walk with drift proceses:

```{r}
set.seed(1)
RW_estimation <-  sign.basic(data=y, p=1, S=S.global, c(0,1), k1 = 1, k2 = 100, shockvar = 1)

A.mean_tab <- as.data.frame(round(RW_estimation$A.mean,3))
colnames(A.mean_tab) <- c("RW1", "RW2")

Sigma.mean_tab <- as.data.frame(round(apply(RW_estimation$Sigma.posterior,1:2,mean),3))
colnames(Sigma.mean_tab) <- c("RW1", "RW2")

paged_table(A.mean_tab)
paged_table(Sigma.mean_tab)

```

Estimating the COVID-volatility model involved designing three functions in `R`:

1.  `v.posterior.mode` takes in the data and numerically computes for the posterior mode of $\theta$, i.e., COVID volatility parameters;

```{r COVID parameters posterior maximization, echo = TRUE}
#| code-fold: true
#| code-summary: "Show code"
v.posterior.mode <- function(data, p=4, k1=1, k2=100, start_date=c(1991,1)){

  v.neglogPost <- function(theta){
    N = ncol(data)
    # p = no. of lags
    K = 1 + p*N
    # forecast horizon
    # h       = 8
    Y       = ts(data[(p+1):nrow(data),], start=start_date, frequency=4)
    T = nrow(Y)
    X       = matrix(1,T,1)
    # nrow(X)
    for (i in 1:p){
      X     = cbind(X,data[(p+1):nrow(data)-i,])
    }
    
    # Calculate MLE for prior 
    ############################################################
    A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
    Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
    
    # Specify prior distribution
    ############################################################
    kappa.1     = k1
    kappa.2     = k2
    kappa.3     = 1
    A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
    A.prior[2:(N+1),] = kappa.3*diag(N)
    V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
    S.prior     = diag(diag(Sigma.hat))
    nu.prior    = N+1
    
    vec <- theta[1:3]
    for (i in 4:12){
      vec <- c(vec, 1 + (theta[3]-1)*theta[4]^(i-3))
    }  
    
    V <- c(ts(rep(1, nrow(Y)-12), c(1991,1), frequency = 4) , vec)    
    
    Y.tilde <- diag(1/V)%*%Y
    X.tilde <- diag(1/V)%*%X
    A.tilde.hat <- solve((t(X.tilde)%*%X.tilde+solve(V.prior)))%*%(t(X.tilde)%*%Y.tilde+solve(V.prior)%*%A.prior)
    epsilon.tilde <-Y.tilde - X.tilde%*%A.tilde.hat
    
    # Log-likelihood      
    logL <- log(prod(V^(-N)))+(-N/2)*log(det(t(X.tilde)%*%X.tilde+solve(V.prior)))+
            (-(T-p+nu.prior)/2)*log(det(S.prior +t(epsilon.tilde)%*%epsilon.tilde + 
            t(A.tilde.hat-A.prior)%*%solve(V.prior)%*%(A.tilde.hat-A.prior)))
    
    # Pareto(1,1) and Beta(3,1.5) priors 
    pareto.a=1
    pareto.b=1
    beta.a=3
    beta.b=1.5
    beta.cons <- 1/beta(beta.a,beta.b)
    
    # Log-prior
    logP <- log((pareto.a*pareto.b^pareto.a)/(theta[1]^(pareto.a+1))*
    (pareto.a*pareto.b^pareto.a)/(theta[2]^(pareto.a+1))*
    (pareto.a*pareto.b^pareto.a)/(theta[3]^(pareto.a+1))*
    beta.cons*theta[4]^(beta.a-1)*(1-theta[4])^(beta.b-1))
    
    # negative log-posterior
    neglogPost <- -(logL+logP)
    
    return(neglogPost)
  }
   
  # numerically minimize the negative log-likelihood
  post.maximizer <- optim(par=c(50, 50, 50, 0.5), fn=v.neglogPost, method="L-BFGS-B", 
                          lower=c(1, 1, 1, 0.0001),
                          upper=c(100,100,100,0.99999), hessian = TRUE)
  
  return(list(maximizer=post.maximizer$par, hessian=post.maximizer$hessian))

}
```

2.  `mh.mcmc` takes in data, the posterior mode of $\theta$, and the inverse Hessian from `v.posterior.mode` to run the above-mentioned Metropolis MCMC algorithm for a specified number of iterations; and

```{r Metropolis-Hastings function, echo = TRUE}
#| code-fold: true
#| code-summary: "Show code"
mh.mcmc <- function(data, p=1, S.mh = S.global, c, W = diag(4), theta.init,
                    k1 = 1, k2 = 100, start_date = c(1991,1)){
 # N = no. of variables
  N = ncol(data)
  # p = no. of lags
  K = 1 + p*N
  # forecast horizon
  # h       = 8
  Y       = ts(data[(p+1):nrow(data),], start=start_date, frequency=4)
  T = nrow(Y)
  X       = matrix(1,T,1)
  # nrow(X)
  for (i in 1:p){
    X     = cbind(X,data[(p+1):nrow(data)-i,])
  }
  

  # Calculate MLE for prior 
  ############################################################
  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  
  # Specify prior distribution
  ############################################################
  kappa.1     = k1
  kappa.2     = k2
  kappa.3     = 1
  A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
  A.prior[2:(N+1),] = kappa.3*diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1
  
  # Metropolis-Hastings 
  ###########################################################
  # v0, v1, v2, rho
  Theta <- matrix(NA,S.mh,4)
  theta_old <- theta.init
  #theta_old <- Theta[nrow(Theta),]
  
  # W <- diag(4)
  set.seed(1)
  for (s in 1:S.mh){

    covid.vec <- function(theta){
      vec <- theta[1:3]
      for (i in 4:12){
        vec <- c(vec, 1 + (theta[3]-1)*theta[4]^(i-3))
      }
      
      return(vec)
    }

    # Covid volatility likelihood kernel
    v.logL <- function(V){
      Y.tilde <- diag(1/V)%*%Y
      X.tilde <- diag(1/V)%*%X
      A.tilde.hat <- solve((t(X.tilde)%*%X.tilde+solve(V.prior)))%*%(t(X.tilde)%*%Y.tilde+solve(V.prior)%*%A.prior)
      epsilon.tilde <-Y.tilde - X.tilde%*%A.tilde.hat

      logL <- log(prod(V^(-N)))+(-N/2)*log(det(t(X.tilde)%*%X.tilde+solve(V.prior)))+
              (-(T-p+nu.prior)/2)*log(det(S.prior +t(epsilon.tilde)%*%epsilon.tilde + 
              t(A.tilde.hat-A.prior)%*%solve(V.prior)%*%(A.tilde.hat-A.prior)))

      return(logL)
    }
  
    # Covid volatility prior
    v.logP <- function(theta, pareto.a=1, pareto.b=1, beta.a=3, beta.b=1.5){
      beta.cons <- 1/beta(beta.a,beta.b)
  
      logP <- log((pareto.a*pareto.b^pareto.a)/(theta[1]^(pareto.a+1))*
      (pareto.a*pareto.b^pareto.a)/(theta[2]^(pareto.a+1))*
      (pareto.a*pareto.b^pareto.a)/(theta[3]^(pareto.a+1))*
       beta.cons*theta[4]^(beta.a-1)*(1-theta[4])^(beta.b-1))
      
      return(logP)
    }

    v_ones <- ts(rep(1, nrow(Y)-12), c(1991,1), frequency = 4) 
    V.old <- c(v_ones, covid.vec(theta_old))    
      
    # New candidate parameters values
    theta_new <- mvrnorm(1, theta_old, c*W)
    V.new <- c(v_ones, covid.vec(theta_new))
    
    # Calculate posteriors 
    v.logpost_old <- v.logL(V.old)+v.logP(theta_old)
    v.logpost_new <- v.logL(V.new)+v.logP(theta_new)
    
    # Posterior ratio
    post.ratio <- exp(v.logpost_new-v.logpost_old)
    
    # Acceptance/rejection alpha
    alpha <- min(1, post.ratio)
    
    u_star <- runif(1)
    
    if (alpha > u_star){
      Theta[s,] <- theta_new
    } else {Theta[s,] <- theta_old}
    
    theta_old <- Theta[s,]  
  }
  
  colnames(Theta) <- c("h0", "h1" , "h2", "rho")

  re <- list(Theta=Theta, 
             AcceptRate = 1 - rejectionRate(as.mcmc(Theta[,1])))
  return(re)
}

```

3.  `sign.extension` takes in data and draws of $\theta$ from `mh.mcmc` to return the posterior draws for the extended model.

```{r Extended model, echo = TRUE}
#| code-fold: true
#| code-summary: "Show code"
sign.extension <- function(data, p=4, S=S.global,  sign.restrictions = c(0, 0, 1, 1, 1, -1, 1, -1, 1),
                           k1=1, k2=100, shockvar = 5, start_date = c(1991,1), Theta.mh){

  # N = no. of variables
  N = ncol(data)
  # p = no. of lags
  K = 1 + p*N
  # forecast horizon
  # h       = 8
  Y       = ts(data[(p+1):nrow(data),], start=start_date, frequency=4)
  T = nrow(Y)
  X       = matrix(1,T,1)
  # nrow(X)
  for (i in 1:p){
    X     = cbind(X,data[(p+1):nrow(data)-i,])
  }
  
  
  covid.vec <- function(theta){
    vec <- theta[1:3]
    for (i in 4:12){
      vec <- c(vec, 1 + (theta[3]-1)*theta[4]^(i-3))
    }
      
    return(vec)
  }
  

  # array of S diag(covid volatility) matrices
  diagV.sqinv <- array(NA, c(nrow(Y),nrow(Y),S))
  
  for (s in 1:S){
    v_ones <- ts(rep(1, nrow(Y)-12), c(1991,1), frequency = 4) 
    diagV.sqinv[,,s] <- diag(c(v_ones, covid.vec(Theta.mh[s,]))^(-2))
  }
  
  # Calculate MLE for prior 
  ############################################################
  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

  # Specify prior distribution
  ############################################################
  kappa.1     = k1
  kappa.2     = k2
  kappa.3     = 1
  A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
  A.prior[2:(N+1),] = kappa.3*diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1
  
  # Posterior draws 
  ############################################################
  Sigma.posterior   = array(NA,c(N,N,S))
  A.posterior       = array (NA,c(K,N,S))
  B0.posterior       = array(NA,c(N,N,S))
  Bplus.posterior       = array(NA,c(N,K,S))
  
  for (s in 1:S){
    V.bar.inv   = t(X)%*%diagV.sqinv[,,s]%*%X + diag(1/diag(V.prior))
    V.bar       = solve(V.bar.inv)
    A.bar       = V.bar%*%(t(X)%*%diagV.sqinv[,,s]%*%Y + diag(1/diag(V.prior))%*%A.prior)
    nu.bar      = nrow(Y) + nu.prior
    S.bar       = S.prior + t(Y)%*%diagV.sqinv[,,s]%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%
                  A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv   = solve(S.bar)
    L                 = t(chol(V.bar))
    
    # RF posterior draws
    Sigma.posterior[,,s] <- solve(rWishart(1, df=nu.bar, Sigma=S.bar.inv)[,,1])
    cholSigma.s     = chol(Sigma.posterior[,,s])
    A.posterior[,,s]       = matrix(mvrnorm(1,as.vector(A.bar), Sigma.posterior[,,s]%x%V.bar),ncol=N)
    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s
    
    # SF posterior draws 
    B0.posterior[,,s]= solve(t(cholSigma.s))
    # Draw Bplus
    Bplus.posterior[,,s] = B0.posterior[,,s]%*%t(A.posterior[,,s])
  }

  # Identification via sign restrictions on theta0
  ############################################################

  # generate corresponding R matrix
  R1 = diag(sign.restrictions)
  
  # storage matrices for Q identified estimates
  i.vec <- c()
  Q.iden   = array(NA,c(N,N,S))
  B0.iden = array(NA,c(N,N,S))
  B1.iden = array(NA,c(N,K,S))
  A.iden = array (NA,c(K,N,S))
  
  for (s in 1:S){
    
    # pick-up a B0 from S
    B0.tilde <- B0.posterior[,,s]
    IR.0.tilde    = solve(B0.tilde)
    B1.tilde      = Bplus.posterior[,,s]
    #IR.1.tilde    = solve(B0.tilde)%*%B1.tilde%*%solve(B0.tilde)

    # Search for appropriate Q 
    sign.restrictions.do.not.hold = TRUE
    i=1
    while (sign.restrictions.do.not.hold){
      X           = matrix(rnorm(N^2),N,N)
      QR          = qr(X, tol = 1e-10)
      Q           = qr.Q(QR,complete=TRUE)
      R           = qr.R(QR,complete=TRUE)
      Q           = t(Q %*% diag(sign(diag(R))))
      B0          = Q%*%B0.tilde
      B1          = Q%*%B1.tilde
      B0.inv      = solve(B0)
      check       = prod(R1 %*% B0.inv %*% diag(N)[,shockvar] >= 0)
      A           = t(solve(B0)%*%B1)

      if (check==1){sign.restrictions.do.not.hold=FALSE}
      i=i+1
    }
    i.vec <- c(i.vec, i)
    Q.iden[,,s] <- Q
    B0.iden[,,s] <- B0
    B0.mean <- apply(B0.iden,1:2,mean)
    B1.iden[,,s] <- B1
    B1.mean <- apply(B1.iden,1:2,mean)
    A.iden[,,s] <- A
    A.mean <- apply(A.iden,1:2,mean)
  }

  re <- list("i" = i.vec, "Q" = Q.iden, "B0"= B0.iden, "B0.mean" = B0.mean,
             "Bplus"= B1.iden, "Bplus.mean" = B1.mean, "A" = A.iden, "A.mean" = A,
             "A.posterior"=A.posterior, "Sigma.posterior"=Sigma.posterior, "Theta"= Theta.mh)
  return(re)
  
}

```

Applying `v.posterior.mode` on the artifcial data yielded a mode of $\theta^{(s=1)}=(1,1,1,0.8)$ which is appropriate given that there is no COVID volatility in generated random walk series. However, the numerical optimization yielded a non-positive definite Hessian which cannot be used to initialize the Metropolis MCMC. As such, $W$ was instead set to an identity matrix.

Running `mh.mcmc` on the artificial data for 15,000 iterations yields the following draws. Unfortunately, the MCMC draws failed to converge into a stationary series within these number of iterations.

```{r RW Metropolis MCMC}
set.seed(1)
RW.post.mode <- v.posterior.mode(y, p = 1)
RW.mcmc <- mh.mcmc(y, c = 0.00001, W = diag(4), 
                   theta.init = RW.post.mode$maximizer, S.mh = 15000)
plot.ts(RW.mcmc$Theta, main = "Metropolis MCMC draws", xlab = "")
#print(RW.post.mode$maximizer) 
#print(RW.post.mode$hessian) 
#RM.theta$AcceptRate
```

Nonetheless, using these draws in the estimation procedure implemented by `sign.extension` still yields correct estimates for $A$ and $\Sigma$, as seen in the results below. Note however, memory capacity of the computer used only allowed for 2000 draws for the extended model.


```{r RW extension, echo = F}
#| code-fold: true
#| code-summary: "Show code"
RW_estimation_ext <-  sign.extension(y, p = 1, S = 2000, 
                                     sign.restrictions=c(0, 0), k1 = 1, 
                                     Theta.mh = RW.mcmc$Theta[13001:15000,], shockvar = 1)

A.mean_tab <- as.data.frame(round(RW_estimation_ext$A.mean,3))
colnames(A.mean_tab) <- c("RW1", "RW2")

Sigma.mean_tab <- as.data.frame(round(apply(RW_estimation_ext$Sigma.posterior,1:2,mean),3))
colnames(Sigma.mean_tab) <- c("RW1", "RW2")
library(rmarkdown)
paged_table(A.mean_tab)
paged_table(Sigma.mean_tab)
```

Estimation of the common stochastic volatility model is conducted using the `sign.stochvol` function below:

```{r SV function, echo = TRUE}
#| code-fold: true
#| code-summary: "Show code"
sign.stochvol <- function(data, p, S,  sign.restrictions,
                           k1=1, k2=100, start_date = c(1991,1), shockvar=5){
  SVcommon.Gibbs.iteration = function(aux, priors){
  # A single iteration of the Gibbs sampler for the SV component
  #
  # aux is a list containing:
  #   Y - a TxN matrix
  #   X - a TxK matrix
  #   H - a Tx1 matrix (update)
  #   h0 - a scalar (update)
  #   sigma.v2 - a scalar (update)
  #   s - a Tx1 matrix (update)
  #   A - a KxN matrix (update)
  #   Sigma - an NxN matrix (update)
  #   sigma2 - a Tx1 matrix (update)
  # 
  # priors is a list containing:
  #   h0.v - a positive scalar
  #   h0.m - a scalar
  #   sigmav.s - a positive scalar
  #   sigmav.nu - a positive scalar
  #   HH - a TxT matrix
  
  # Auxiliary normal mixture approximation of the chi-square
  T             = dim(aux$Y)[1]
  N             = dim(aux$Y)[2]
  alpha.st      = c(1.92677,1.34744,0.73504,0.02266,0-0.85173,-1.97278,-3.46788,-5.55246,-8.68384,-14.65000)
  sigma.st      = c(0.11265,0.17788,0.26768,0.40611,0.62699,0.98583,1.57469,2.54498,4.16591,7.33342)
  pi.st         = c(0.00609,0.04775,0.13057,0.20674,0.22715,0.18842,0.12047,0.05591,0.01575,0.00115)
  
  Lambda        = solve(chol(aux$Sigma[,,s]))
  Z             = rowSums( ( aux$Y - aux$X %*% aux$A[,,s] ) %*% Lambda ) / sqrt(N)
  Y.tilde       = as.vector(log((Z + 0.0000001)^2))
  Ytilde.alpha  = as.matrix(Y.tilde - alpha.st[as.vector(aux$s[,s])])
  
  # sampling initial condition
  ############################################################
  V.h0.bar      = 1/((1 / priors$h0.v) + (1 / aux$sigma.v2[(s)]))
  m.h0.bar      = V.h0.bar*((priors$h0.m / priors$h0.v) + (aux$H[1,s] / aux$sigma.v2[s]))
  h0.draw       = rnorm(1, mean = m.h0.bar, sd = sqrt(V.h0.bar))
  aux$h0[s]        = h0.draw
  
  # sampling sigma.v2
  ############################################################
  sigma.v2.s    = priors$sigmav.s + sum(c(aux$H[1,s] - aux$h0[s], diff(aux$H[,s]))^2)
  sigma.v2.draw = sigma.v2.s / rchisq(1, priors$sigmav.nu + T)
  aux$sigma.v2[s]  = sigma.v2.draw
  
  # sampling auxiliary states
  ############################################################
  Pr.tmp        = simplify2array(lapply(1:10,function(x){
    dnorm(Y.tilde, mean = as.vector(aux$H[,(s)] + alpha.st[x]), sd = sqrt(sigma.st[x]), log = TRUE) + log(pi.st[x])
  }))
  Pr            = t(apply(Pr.tmp, 1, function(x){exp(x - max(x)) / sum(exp(x - max(x)))}))
  s.cum         = t(apply(Pr, 1, cumsum))
  r             = matrix(rep(runif(T), 10), ncol = 10)
  ss            = apply(s.cum < r, 1, sum) + 1
  aux$s[,s]         = as.matrix(ss)
  
  
  # sampling log-volatilities using functions for tridiagonal precision matrix
  ############################################################
  Sigma.s.inv   = diag(1 / sigma.st[as.vector(aux$s[,s])])
  D.inv         = Sigma.s.inv + (1 / aux$sigma.v2[s]) * priors$HH
  b             = as.matrix(Ytilde.alpha / sigma.st[as.vector(aux$s[,s])] + (aux$h0[s]/aux$sigma.v2[s])*diag(T)[,1])
  lead.diag     = diag(D.inv)
  sub.diag      = mgcv::sdiag(D.inv, -1)
  D.chol        = mgcv::trichol(ld = lead.diag, sd = sub.diag)
  D.L           = diag(D.chol$ld)
  mgcv::sdiag(D.L,-1) = D.chol$sd
  x             = as.matrix(rnorm(T))
  a             = forwardsolve(D.L, b)
  draw          = backsolve(t(D.L), a + x)
  aux$H[,s]         = as.matrix(draw)
  aux$sigma2[,s]    = as.matrix(exp(draw))

  return(aux)
  }
  
  
  # Define Y and X matrices
  ############################################################
  # N = no. of variables
  N = ncol(data)
  # p = no. of lags
  K = 1 + p*N
  # forecast horizon
  
  Y       = ts(data[(p+1):nrow(data),], start=start_date, frequency=4)
  X       = matrix(1,nrow(Y),1)
  # nrow(X)
  for (i in 1:p){
    X     = cbind(X,data[(p+1):nrow(data)-i,])
  }
  T = nrow(Y)
  # Calculate MLE
  ############################################################
  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  
  # Specify prior distribution
  ############################################################
  kappa.1     = k1
  kappa.2     = k2
  kappa.3     = 1
  A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
  A.prior[2:(N+1),] = kappa.3*diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1
  
  HH        = 2*diag(T)
  sdiag(HH,-1) =  -1
  sdiag(HH,1) =  -1

  SV.basic.priors <- list(
    h0.v = 1,
    h0.m = 0,
    sigmav.s = 1,
    sigmav.nu = 1,
    HH = HH)  
    
  H           = matrix(1,T,S)
  sdiag(H,-1) =  -1

  aux <- list(
    Y = Y,
    X = X,
    H = H,
    h0 = matrix(0,S),
    sigma.v2 = matrix(1,S),
    s = matrix(1,T,S),
    A = array(NA, c(K,N,S)),
    Sigma = array(NA, c(N,N,S)),
    sigma2 = matrix(1,T,S)
    )

  V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))
  V.bar       = solve(V.bar.inv)
  A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
  nu.bar      = nrow(Y) + nu.prior
  S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
  S.bar.inv   = solve(S.bar)
  L                 = t(chol(V.bar))

  Sigma.init <- solve(rWishart(1, df=nu.bar, Sigma=S.bar.inv)[,,1])
  cholSigma.s     = chol(Sigma.init)
  A.init       = matrix(mvrnorm(1,as.vector(A.bar), Sigma.init%x%V.bar),ncol=N)
  A.init= A.bar + L%*%A.init%*%cholSigma.s
  
  aux$A[,,1] <- A.init 
  aux$Sigma[,,1] <- Sigma.init 

  # Posterior draws with SV
  ############################################################
  Sigma.posterior   = array(NA,c(N,N,S))
  A.posterior       = array (NA,c(K,N,S))
  B0.posterior       = array(NA,c(N,N,S))
  Bplus.posterior       = array(NA,c(N,K,S))

  for (s in 1:S){
    aux <- SVcommon.Gibbs.iteration(aux, SV.basic.priors)
    
    V.bar.inv   = t(X)%*%diag(1/as.vector(aux$sigma2[,s]))%*%X + diag(1/diag(V.prior))
    V.bar       = solve(V.bar.inv)
    A.bar       = V.bar%*%(t(X)%*%diag(1/as.vector(aux$sigma2[,s]))%*%Y + diag(1/diag(V.prior))%*%A.prior)
    nu.bar      = nrow(Y) + nu.prior
    S.bar       = S.prior + t(Y)%*%diag(1/as.vector(aux$sigma2[,s]))%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv   = solve(S.bar)
    L                 = t(chol(V.bar))

    # RF posterior draws
    Sigma.posterior[,,s] <- solve(rWishart(1, df=nu.bar, Sigma=S.bar.inv)[,,1])
    cholSigma.s     = chol(Sigma.posterior[,,s])
    A.posterior[,,s]       = matrix(mvrnorm(1,as.vector(A.bar), Sigma.posterior[,,s]%x%V.bar),ncol=N)
    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s
    
    # SF posterior draws 
    B0.posterior[,,s]= solve(t(cholSigma.s))
    # Draw Bplus
    Bplus.posterior[,,s] = B0.posterior[,,s]%*%t(A.posterior[,,s])
    
    # draw SV
    aux$A[,,s] <- A.posterior[,,s]
    aux$Sigma[,,s] <- Sigma.posterior[,,s]
    
    if (s < S){
    aux$H[,(s+1)] <- aux$H[,s]
    aux$h0[(s+1)] <- aux$h0[s]
    aux$sigma.v2[(s+1)] <- aux$sigma.v2[s]
    aux$s[,(s+1)] <- aux$s[,s]
    aux$A[,,(s+1)] <- aux$A[,,s]
    aux$Sigma[,,(s+1)] <- aux$Sigma[,,s]
    aux$sigma2[,(s+1)] <- aux$sigma2[,s]
    }
  }

  # Identification via sign restrictions 
  ############################################################
  
  # Generate corresponding R matrix
  R1 = diag(sign.restrictions)
  
  # Storage matrices for Q identified estimates
  i.vec <- c()
  Q.iden   = array(NA,c(N,N,S))
  B0.iden = array(NA,c(N,N,S))
  B1.iden = array(NA,c(N,K,S))
  A.iden = array (NA,c(K,N,S))

  for (s in 1:S){
    
    # pick-up a B0 from S
    B0.tilde <- B0.posterior[,,s]
    IR.0.tilde    = solve(B0.tilde)
    B1.tilde      = Bplus.posterior[,,s]
    #IR.1.tilde    = solve(B0.tilde)%%B1.tilde%%solve(B0.tilde)

    # Search for appropriate Q 
    sign.restrictions.do.not.hold = TRUE
    i=1
    while (sign.restrictions.do.not.hold){
      X           = matrix(rnorm(N^2),N,N)
      QR          = qr(X, tol = 1e-10)
      Q           = qr.Q(QR,complete=TRUE)
      R           = qr.R(QR,complete=TRUE)
      Q           = t(Q %*% diag(sign(diag(R))))
      B0          = Q%*%B0.tilde
      B1          = Q%*%B1.tilde
      B0.inv      = solve(B0)
      check       = prod(R1 %*% B0.inv %*% diag(N)[,shockvar] >= 0)
      A           = t(solve(B0)%*%B1)

      if (check==1){sign.restrictions.do.not.hold=FALSE}
      i=i+1
    }
    i.vec <- c(i.vec, i)
    Q.iden[,,s] <- Q
    B0.iden[,,s] <- B0
    B0.mean <- apply(B0.iden,1:2,mean)
    B1.iden[,,s] <- B1
    B1.mean <- apply(B1.iden,1:2,mean)
    A.iden[,,s] <- A
    A.mean <- apply(A.iden,1:2,mean)
    

  }
  re <- list("i" = i.vec, "Q" = Q.iden, "B0"= B0.iden, "B0.mean" = B0.mean,
             "Bplus"= B1.iden, "Bplus.mean" = B1.mean, "A" = A.iden, "A.mean" = A,
             "A.posterior"=A.posterior, "Sigma.posterior"=Sigma.posterior, 
             "sigma2" = aux$sigma2
             )
  return(re)
}
```

Applying the `sign.stochvol` on the random walk with drift data yields an appropriate identity matrix estimate for $A$. However, the product of the average $\sigma^2$ and $\Sigma$ estimates do not yield an identity matrix. This may be due to poor estimation of $\sigma^2$ as the artificial data does not provide any real signals (i.e., there is no actual conditional heteroskedasticity).

```{r RW SV}
#| code-fold: true
#| code-summary: "Show code"
RW_SV <- sign.stochvol(data=y, p=1, S=2000, c(0,1), k1 = 1, k2 = 100, shockvar = 1)

A.mean_tab <- as.data.frame(round(apply(RW_SV$A.posterior,1:2,mean),3))
colnames(A.mean_tab) <- c("RW1", "RW2")

Sigma.mean_tab <- as.data.frame(round(apply(RW_SV$Sigma.posterior,1:2,mean),3))
colnames(Sigma.mean_tab) <- c("RW1", "RW2")

paged_table(A.mean_tab)
paged_table(Sigma.mean_tab)
print(mean(apply(RW_SV$sigma2,2,mean)))
```

## Empirical investigation

This section investigates the effect of an exogenous shock to public investment, represented by the public gross fixed capital formation (GFCF, i.e., the 5th variable in the $Y$). These will be measured through impulse response functions (IRFs) which capture the dynamic causal effects of an exogenous shock $u_t$ on the variables in the system.

Given the $VAR(1)$ representation of the model: $$Y_t = \textbf{A}Y_{t-1} + E_t$$ $$=E_t+\textbf{A}E_{t-1}+\textbf{A}^2E_{t-2}+...$$ Transforming back to a $VAR(p)$ representation using the $J$ matrix:

$$J=\big[I_n\quad 0_{N\times N(p-1)}\big]$$

$$y_t =JY_t=JE_t+J\textbf{A}J'JE_{t-1}+J\textbf{A}^2J'JE_{t-2}+...$$ $$=\varepsilon_t+J\textbf{A}J'\varepsilon_{t-1}+J\textbf{A}^2J'\varepsilon_{t-2}+...$$ Substituting $\varepsilon_t=Bu_t$:

$$y_t =Bu_t+J\textbf{A}J'Bu_{t-1}+J\textbf{A}^2J'Bu_{t-2}+...$$ $$=\Theta_0u_t+\Theta_1u_{t-1}+\Theta_2u_{t-2}+...$$Where matrix $\frac{\partial y_{t+i}}{\partial u_t}=\Theta_i$ is the IRF which characterizes the effect of the variables $y_t$, $i$ periods from impact.

The above derivations are implemented in the function `irf.plot` below, which takes in draws of $A$ and $B_0$ and does the necessary transformations estimate and plot the impluse response functions for a shock on a specified variable.

```{r IRF plot function, echo = TRUE}
#| code-fold: true
#| code-summary: "Show code"
irf.plot <- function(A.posterior, B0.posterior, shock.var, p=4, h = 12,
                     varnames = varname_vec){
  # Define colors
  ############################################################
  mcxs1  = "#05386B"
  mcxs2  = "#379683"
  mcxs3  = "#5CDB95"
  mcxs4  = "#8EE4AF"
  mcxs5  = "#EDF5E1"
  purple = "#b02442"
  
  mcxs1.rgb   = col2rgb(mcxs1)
  mcxs1.shade1= rgb(mcxs1.rgb[1],mcxs1.rgb[2],mcxs1.rgb[3], alpha=120, maxColorValue=255)
  mcxs2.rgb   = col2rgb(mcxs2)
  mcxs2.shade1= rgb(mcxs2.rgb[1],mcxs2.rgb[2],mcxs2.rgb[3], alpha=120, maxColorValue=255)
  
  # Impulse response functions
  ############################################################
  N <- dim(A.posterior)[2]
  S <- dim(A.posterior)[3]
  
  # transform B0 matrices to B
  B.posterior <- array(NA, c(N,N,S))
  for (s in 1:S){
  B.posterior[,,s] <- solve(B0.posterior[,,s])
  }
  
  IRF.posterior     = array(NA,c(N,N,h+1,S))
  IRF.inf.posterior = array(NA,c(N,N,S))
  J                 = cbind(diag(N),matrix(0,N,N*(p-1)))
  
  for (s in 1:S){
    # define A matrix in VAR(1) representation
    A.bold          = rbind(t(A.posterior[2:(1+N*p),,s]),cbind(diag(N*(p-1)),matrix(0,N*(p-1),N)))
    IRF.inf.posterior[,,s]          = J %*% solve(diag(N*p)-A.bold) %*% t(J) %*% B.posterior[,,s]
    A.bold.power    = A.bold
    for (i in 1:(h+1)){
      if (i==1){
        IRF.posterior[,,i,s]        = B.posterior[,,s]
      } else {
        IRF.posterior[,,i,s]        = J %*% A.bold.power %*% t(J) %*% B.posterior[,,s]
        A.bold.power                = A.bold.power %*% A.bold
      }
    }
  }
  
  # save IRFs
  save(IRF.posterior,IRF.inf.posterior, file="irf-k1.RData")

  # IRF plots GFCF
  ############################################################
  IRFs.k1           = apply(IRF.posterior[,shock.var,,],1:2,mean)
  IRFs.inf.k1       = apply(IRF.inf.posterior[,shock.var,],1,mean)
  rownames(IRFs.k1) = varnames
  
  IRFs.k1.hdi    = apply(IRF.posterior[,shock.var,,],1:2,hdi, credMass=0.68)
  hh          = 1:(h+1)
  
  par(mfrow=c(3,3), mar=c(3,3,2,2),cex.axis=1.5, cex.lab=1.5)
  for (n in 1:N){
    ylims     = range(IRFs.k1[n,hh],IRFs.k1.hdi[,n,hh])
    plot(hh,IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="3 years",
         main=rownames(IRFs.k1)[n])
    abline(h = 0, col = "firebrick")
    axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
    polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh],IRFs.k1.hdi[2,n,(h+1):1]),
            col=mcxs1.shade1,border=mcxs1.shade1)
    lines(hh, IRFs.k1[n,hh],lwd=2,col=mcxs1)
  }
}

```

### Basic model estimation and IRFs

The basic model is used to investigate the dynamic effects of shock to public investment on the economic system. Identification proceeds via the following sign restrictions on $\Theta_0$:

$$f(B_0,B_+)=\Theta_0=B=\begin{bmatrix}
* & * & * & * & * & * & * & * & *\\
* & * & * & * & * & * & * & * & *\\
+ & * & * & * & * & * & * & * & *\\
+ & * & * & * & * & * & * & * & *\\
+ & * & * & * & * & * & * & * & *\\
+ & * & * & * & * & * & * & * & *\\
+ & * & * & * & * & * & * & * & *\\
- & * & * & * & * & * & * & * & *\\
+ & * & * & * & * & * & * & * & *\\
\end{bmatrix}$$

which corresponds to the following $\textbf{R}_5$ matrix: $$\textbf{R}_5=\begin{bmatrix}
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & -1 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\
\end{bmatrix}$$

$$\textbf{R}_5f(B_0,B_+)e_5\geq0$$

Where the identifying fiscal policy impulse vector is: $$\text{diag}(\textbf{R}_5)=[0\quad0\quad1\quad1\quad1\quad1\quad1\quad-1\quad1]$$ $\text{diag}(\textbf{R}_5)$ represents an exogenous shock to public investment to which other fiscal variables respond positively and are accommodated by monetary policy. This scenario is applicable, for example during situations where the Australian economy is experiencing low inflation but sluggish growth which the government may want to stimulate in the short-to-medium term. Variables for unemployment and output, the first and second variables in $y_t$, respectively, are left unrestricted as these are the variables whose responses are of interest.

```{r Basic model estimation, echo = TRUE}
#| code-fold: true
#| code-summary: "Show code"

basic.model <- sign.basic(lnY.df_num,p=4, S=S.global, c(0, 0, 1, 1, 1, 1, 1, -1, 1), k1 = 1, shockvar=5)

irf.plot(A.posterior = basic.model$A.posterior, B0.posterior = basic.model$B0, shock.var = 5)
```

As seen in the above plots, an exogenous shock to public investment (i.e., government GFCF) at period $h=0$ is estimated to increase real GDP in subsequent periods, with the majority of estimated real GDP impulse responses at $h>0$ above the zero level. This expansion in output is accompanied by a decrease in unemployment. These results are consistent with the Keynesian economic theory that expansionary fiscal policy (through increased spending) increases output and employment. Note that these improvements to GDP and unemployment occur despite increases in tax revenue.

### COVID volatility model estimation and IRFs

The same sign restrictions are used for identification in the COVID volatility model. Functions `v.posterior` and `mh.mcmc` are implemented to draw vector $H$ based on the macroeconomic data. As seen in the plots below, the Metropolis MCMC also failed to converge to stationarity after 20,000 draws.

```{r Extended model MCMC, echo = TRUE}
#| code-fold: true
#| code-summary: "Show code"
extension.post.mode <- v.posterior.mode(lnY.df_num, p = 4, k1 = 1, k2 = 100)
set.seed(1)
extension.mcmc <-mh.mcmc(lnY.df_num, p=4, c=0.000228, W = solve(extension.post.mode$hessian), 
                theta.init = extension.post.mode$maximizer, k1 = 1, S.mh = 2*S.global)
plot.ts(extension.mcmc$Theta, main = "Metropolis MCMC draws", xlab = "")

```

The resulting IRF plots from the extended model mimic the basic model results where an exogenous shock to GFCF results in higher output and lower unemployment rates. However, the bands surrounding the mean line of the IRFs have become slightly tighter, as accounting for COVID-induced conditional heteroskedasticity effectively down-weights the heightened volatility of observations during the pandemic.

```{r Extended model plots, echo = TRUE}
#| code-fold: true
#| code-summary: "Show code"
extended.model <- sign.extension(lnY.df_num, p = 4, S = S.global, sign.restrictions=c(0, 0, 1, 1, 1, 1, 1, -1, 1), k1 = 1, 
                                 Theta.mh = extension.mcmc$Theta[(S.global+1):(2*S.global),])

irf.plot(A.posterior = extended.model$A.posterior, B0.posterior = extended.model$B0, shock.var = 5)
```

Moreover, estimates suggest that conditional volatility peaked during Q2 2020, sharply declined in the subsequent quarter and linearly decaying thereafter. These results do not seem to be realistic.
```{r COVID volatility plots, echo = TRUE}
#| code-fold: true
#| code-summary: "Show code"
mcxs1  = "#05386B"
mcxs2  = "#379683"
mcxs3  = "#5CDB95"
mcxs4  = "#8EE4AF"
mcxs5  = "#EDF5E1"
purple = "#b02442"

mcxs1.rgb   = col2rgb(mcxs1)
mcxs1.shade1= rgb(mcxs1.rgb[1],mcxs1.rgb[2],mcxs1.rgb[3], alpha=100, maxColorValue=255)
mcxs2.rgb   = col2rgb(mcxs2)
mcxs2.shade1= rgb(mcxs2.rgb[1],mcxs2.rgb[2],mcxs2.rgb[3], alpha=100, maxColorValue=255)
mcxs3.rgb   = col2rgb(mcxs3)
mcxs3.shade1= rgb(mcxs3.rgb[1],mcxs3.rgb[2],mcxs3.rgb[3], alpha=100, maxColorValue=255)

H <- array(NA,c(12,10000))

covid.vec <- function(theta){
  vec <- theta[1:3]
  for (i in 4:12){
    vec <- c(vec, 1 + (theta[3]-1)*theta[4]^(i-3))
  }
  return(vec)
}
  
for (s in 1:S.global){
  H[,s] <- covid.vec(extension.mcmc$Theta[s,])
}

sv.mean <- apply(H,1,mean)
sv.hdi  = apply(H,1,hdi, credMass=.9)

Y       = ts(lnY.df_num[(4+1):nrow(lnY.df_num),], start=c(1991,1), frequency=4)
x.date <- as.Date(time(Y))[117:128]

plot(x=x.date, y=sv.mean, type="l", col=mcxs1, ylim =c(0,7), bty="n", lwd = 2,
     ylab = "h",xlab = "", main = "Conditional volatility since COVID")
polygon(c(x.date[1:12],x.date[12:1]),
        c(sv.hdi[1,],sv.hdi[2,12:1]), 
        col=mcxs2.shade1, border=mcxs2.shade1)
```

### Common stochastic volatility model estimation and IRFs

Lastly, the common SV model was ran on the macroeconomic data and yields the following IRFs. By and large, the general location of the mean lines are similar to the basic and COVID-volatility model, though the estimated effect of fiscal policy on output has become negative to near zero. We also note that surrounding bands around the mean have become much narrower in this model, driven by the much smaller variance of the parameters (see Appendix). 
```{r SV IRF, echo = TRUE}
#| code-fold: true
#| code-summary: "Show code"

set.seed(1)
SV <- sign.stochvol(data=lnY.df_num, p=4, S=(2*S.global), c(0, 0, 1, 1, 1, 1, 1, -1, 1), k1 = 1, k2 = 100, shockvar = 5)

irf.plot(A.posterior = SV$A.posterior[,,(S.global+1):(2*S.global)], B0.posterior = SV$B0[,,(S.global+1):(2*S.global)], shock.var = 5)
```

Moreover, the $\sigma$ estimates provide a more realistic picture of the conditional volatility across the time series. As seen in the plot below, the common SV model not only captures the heightened volatility during COVID, but also the spikes in uncertainty during the Asian financial crisis and the global financial crisis.
```{r SV plot, echo = T}
#| code-fold: true
#| code-summary: "Show code"

sv.mean <- apply(sqrt(SV$sigma2[,(S.global+1):(2*S.global)]),1,mean)
sv.hdi  = apply(sqrt(SV$sigma2[,(S.global+1):(2*S.global)]),1,hdi, credMass=.9)

Y       = ts(lnY.df_num[(4+1):nrow(lnY.df_num),], start=c(1991,1), frequency=4)
x.date <- as.Date(time(Y))

plot(x=x.date, y=sv.mean, type="l", col=mcxs1, ylim =c(0,100), bty="n", lwd = 2,
     ylab = expression(sigma),xlab = "", main = "Conditional common volatility")
polygon(c(x.date[1:nrow(Y)],x.date[nrow(Y):1]),
        c(sv.hdi[1,],sv.hdi[2,nrow(Y):1]), 
        col=mcxs2.shade1, border=mcxs2.shade1)
lines(x=x.date,sv.hdi[1,], lwd = 1, col = mcxs2)
lines(x=x.date,sv.hdi[2,], lwd = 1, col = mcxs2)
abline(v=as.Date("1998-07-01"), col = "grey25", lty = "dotted", lwd = 1.25)
abline(v=as.Date("2009-07-01"), col = "grey25", lty = "dotted", lwd = 1.25)
abline(v=as.Date("2020-04-01"), col = "grey25", lty = "dotted", lwd = 1.25)
text("AFC",x=as.Date("1997-01-01"),y=95,col="black")
text("GFC",x=as.Date("2008-01-01"),y=95,col="black")
text("COVID",x=as.Date("2018-04-01"),y=95,col="black")
```
## Conclusion and possible extensions

Results across three models suggest that expansionary fiscal policy has some negative effect on unemployment, particularly in the short-to-medium term, which is consistent with the Keynesian framework. The basic and COVID volatility models also corroborate the expected positive effect on output, though this is dampened by the results from the common SV model. Lastly, the results suggest that the simple SV model may be a more practical and effective means of estimating conditional macroeconomic volatility, as it is able to capture heteroskedasticity across the time series (not just during COVID) while also being much easier to implement. 

We note that identification via sign restrictions was not able to successfully isolate the shocks of any one particular fiscal instrument. As such, an alternative identification strategy that mixes both exclusion and sign restrictions may be more appropriate for this objective. Moreover, given that the Metropolis MCMC failed to converge for both the artificial and actual data, the use of a Hamiltonian MCMC may be explored as this may be more computationally stable and successful in implementing the COVID volatility model.

## Appendix: Sample histograms of selected parameters

```{r Appendix basic, echo = F}
breaks.a = 100

par(mfrow=c(1,3))
hist(basic.model$A.posterior[3,5,],
     main=expression(paste("Basic model: Distribution of ",A[35], " draws")),
     freq=FALSE,
     breaks = breaks.a,
     lty = "blank",
     col = "plum4",
     xlab = "")

hist(basic.model$Sigma.posterior[5,5,],
     main=expression(paste("Basic model: Distribution of ",Sigma[55], " draws")),
     freq=FALSE,
     breaks = breaks.a,
     lty = "blank",
     col = "dodgerblue2",
     xlab = "")

hist(basic.model$B0[2,5,],
     main=expression(paste("Basic model: Distribution of ",B[0.35], " draws")),
     freq=FALSE,
     breaks = breaks.a,
     lty = "blank",
     col = "salmon2",
     xlab = "")

par(mfrow=c(1,3))
hist(extended.model$A.posterior[3,5,],
     main=expression(paste("COVID model: Distribution of ",A[35], " draws")),
     freq=FALSE,
     breaks = breaks.a,
     lty = "blank",
     col = "plum4",
     xlab = "")

hist(extended.model$Sigma.posterior[5,5,],
     main=expression(paste("COVID model: Distribution of ",Sigma[55], " draws")),
     freq=FALSE,
     breaks = breaks.a,
     lty = "blank",
     col = "dodgerblue2",
     xlab = "")

hist(extended.model$B0[2,5,],
     main=expression(paste("COVID model: Distribution of ",B[0.35], " draws")),
     freq=FALSE,
     breaks = breaks.a,
     lty = "blank",
     col = "salmon2",
     xlab = "")

par(mfrow=c(1,3))
hist(SV$A.posterior[3,5,(S.global+1):(2*S.global)],
     main=expression(paste("SV model: Distribution of ",A[35], " draws")),
     freq=FALSE,
     breaks = breaks.a,
     lty = "blank",
     col = "plum4",
     xlab = "")

hist(SV$Sigma.posterior[5,5,(S.global+1):(2*S.global)],
     main=expression(paste("SV model: Distribution of ",Sigma[55], " draws")),
     freq=FALSE,
     breaks = breaks.a,
     lty = "blank",
     col = "dodgerblue2",
     xlab = "")

hist(SV$B0[2,5,(S.global+1):(2*S.global)],
     main=expression(paste("SV model: Distribution of ",B[0.35], " draws")),
     freq=FALSE,
     breaks = breaks.a,
     lty = "blank",
     col = "salmon2",
     xlab = "")
```


## References {.unnumbered}
